{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":6335979,"sourceType":"datasetVersion","datasetId":3647475},{"sourceId":6345009,"sourceType":"datasetVersion","datasetId":3653435},{"sourceId":6355055,"sourceType":"datasetVersion","datasetId":3655812},{"sourceId":6386005,"sourceType":"datasetVersion","datasetId":3653455},{"sourceId":6394610,"sourceType":"datasetVersion","datasetId":3686354}],"dockerImageVersionId":30528,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade tensorflow-io\nimport tensorflow as tf\nimport tensorflow_io as tfio\ntf.__version__","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd,numpy as np,os,plotly.graph_objects as go,tensorflow as tf\nfrom tensorflow import keras\nfrom scipy.ndimage import zoom\nfrom plotly.subplots import make_subplots","metadata":{"execution":{"iopub.status.busy":"2023-09-21T21:55:13.770698Z","iopub.execute_input":"2023-09-21T21:55:13.771294Z","iopub.status.idle":"2023-09-21T21:55:13.880721Z","shell.execute_reply.started":"2023-09-21T21:55:13.771260Z","shell.execute_reply":"2023-09-21T21:55:13.879795Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import os,numpy as np\nx_train=[];y_train=[];w_train=[];x_val=[];y_val=[];w_val=[]\nfor i in np.sort(os.listdir('/kaggle/input/zscores/train/X/')):\n    temp=np.load('/kaggle/input/zscores/train/X/'+i)\n    x_train.append(temp[:576])\n\nfor i in np.sort(os.listdir('/kaggle/input/zscores/train/Y/')):\n    temp=np.load('/kaggle/input/zscores/train/Y/'+i)\n    temp32=zoom(temp,[2,2,2]+[1],mode='reflect',order=1).astype('float32')\n    y_train.append(temp32)\n\nfor i in np.sort(os.listdir('/kaggle/input/zscores/train/W/')):\n    temp=np.load('/kaggle/input/zscores/train/W/'+i)\n    temp32=zoom(temp,[2,2,2]+[1],mode='reflect',order=1).astype('float32')\n    w_train.append(temp32)\n\nfor i in np.sort(os.listdir('/kaggle/input/zscores/val/X/')):\n    temp=np.load('/kaggle/input/zscores/val/val/X/'+i)\n    x_val.append(temp[:576])\n\nfor i in np.sort(os.listdir('/kaggle/input/zscores/val/val/Y/')):\n    temp=np.load('/kaggle/input/zscores/val/val/Y/'+i)\n    temp32=zoom(temp,[2,2,2]+[1],mode='reflect',order=1).astype('float32')\n    y_val.append(temp32)\n\nfor i in np.sort(os.listdir('/kaggle/input/zscores/val/val/W/')):\n    temp=np.load('/kaggle/input/zscores/val/val/W/'+i)\n    temp32=zoom(temp,[2,2,2]+[1],mode='reflect',order=1).astype('float32')\n    w_val.append(temp32)","metadata":{"execution":{"iopub.status.busy":"2023-09-21T21:56:32.694559Z","iopub.execute_input":"2023-09-21T21:56:32.694916Z","iopub.status.idle":"2023-09-21T22:00:10.007139Z","shell.execute_reply.started":"2023-09-21T21:56:32.694887Z","shell.execute_reply":"2023-09-21T22:00:10.006172Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"w_train[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-09-21T22:00:26.374707Z","iopub.execute_input":"2023-09-21T22:00:26.375045Z","iopub.status.idle":"2023-09-21T22:00:26.381464Z","shell.execute_reply.started":"2023-09-21T22:00:26.375018Z","shell.execute_reply":"2023-09-21T22:00:26.380471Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"(32, 32, 16, 2)"},"metadata":{}}]},{"cell_type":"code","source":"from tensorflow.keras.callbacks import EarlyStopping\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Concatenate, MaxPooling3D, UpSampling3D, Multiply, Dropout, ZeroPadding2D, Flatten, Conv2D, Conv3D, Conv2DTranspose, Conv3DTranspose, BatchNormalization, Activation, Reshape, Concatenate, Dense\nfrom tensorflow.keras.optimizers import Adamax, RMSprop\nimport numpy as np\n\n\ndef weighted_mse(y_true, y_pred, weights):\n    sq_errors = tf.math.square(y_pred-y_true)\n    weighted_sq_errors = sq_errors*weights\n    return weighted_sq_errors\n\n\ndef fdnet(input_shape, output_shape, n_filters=64, kernel_size=5, fc_dropout=0.2, ac1='tanh', ac='relu', learning_rate=0.0001):\n\n    input_meas = keras.Input(shape=input_shape)\n    y_true = keras.Input(shape=output_shape)\n    weights = keras.Input(shape=output_shape)\n\n    x = Flatten()(input_meas)\n    x = Dense(np.product(output_shape), activation=ac1)(x)\n    x = Dropout(fc_dropout)(x)\n    x = Reshape(output_shape)(x)\n\n    x = Conv3D(filters=n_filters, kernel_size=kernel_size, activation=ac, padding='same')(x)\n    x = MaxPooling3D((2,2,2))(x)\n    x = Conv3D(filters=n_filters, kernel_size=kernel_size, activation=ac, padding='same')(x)\n    x = Conv3DTranspose(n_filters, (2, 2, 2), strides=2, padding='same')(x)\n    x = Conv3D(filters=n_filters, kernel_size=kernel_size, activation=ac, padding='same')(x)\n\n    y_pred = Conv3D(filters=2, kernel_size=1, strides=1, padding='same', activation=ac)(x)\n\n    model = keras.Model([input_meas, y_true, weights], y_pred)\n    model.add_loss(weighted_mse(y_true, y_pred, weights))\n    model.compile(optimizer=Adamax(learning_rate = learning_rate))\n\n\n    final_model = keras.Model(input_meas, y_pred)\n\n    return model, final_model\n\n\nmodel_name='zscore_fdnet_undersampled_Amp32_Adamax'\nmodel, final_model = fdnet(x_train[0].shape,y_train[0].shape)\n\nhistory = model.fit([np.array(x_train),np.array(y_train),np.array(w_train)],None,\n                    batch_size=64,verbose = 3,epochs=1000,\n                    validation_data=[[np.array(x_val),np.array(y_val),np.array(w_val)],None],\n                    callbacks = [EarlyStopping(monitor='val_loss',\n                                mode='min',\n                                restore_best_weights=True,\n                                verbose=3,patience=60)])\n\n\n\nmodel.save('/kaggle/working/'+model_name)\nmodel.save('/kaggle/working/zscore_fdnet_undersampled_Amp32_Adamax.h5')\ntrain_losses = history.history['loss']\nval_losses = history.history['val_loss']\n#vis_train_loss(history,title='losses',savename='/kaggle/working/models/training_losses')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_model.summary()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.figure(figsize=(10, 6))\nplt.plot(train_losses,label='Training loss',color='skyblue')\nplt.plot(val_losses,label='Validation loss',color='orange')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Training Loss vs Validation Loss')\nplt.legend()\nplt.savefig('loss_plot_Amp32')\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def save_losses_to_file(train_losses, val_losses, filename):\n    with open(filename, 'w', newline='') as csvfile:\n        fieldnames = ['Epoch', 'Train Loss', 'Validation Loss']\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        writer.writeheader()\n\n        for epoch, (train_loss, val_loss) in enumerate(zip(train_losses, val_losses), start=1):\n            writer.writerow({'Epoch': epoch, 'Train Loss': train_loss, 'Validation Loss': val_loss})\n            \nsave_losses_to_file(train_losses, val_losses, 'losses_amp32.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!zip -r zscore_fdnet_undersampled_Amp32_Adamax.zip '/kaggle/working/zscore_fdnet_undersampled_Amp32_Adamax'\nfrom IPython.display import FileLink\nFileLink('zscore_fdnet_undersampled_Amp32_Adamax.zip')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('/kaggle/input/zscores/factors.p', 'rb') as f:\n    factor = pickle.load(f)\nfactors={}\nfactors['mean']=factor[0]\nfactors['std']=factor[1]\nfactors['mua_min']=factor[2]\nfactors['mua_range']=factor[3]\nfactors['mus_min']=factor[4]\nfactors['mus_range']=factor[5]\nfactors['dwn_factor']=factor[6]\nfactors['roi_factor']=factor[7]\nfactors","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U Kaleido","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import scipy.io, numpy as np,plotly.io as pio\nfrom scipy.ndimage import zoom\nfrom plotly.subplots import make_subplots\nfrom skimage.metrics import structural_similarity as compare_ssim\nimport matplotlib.pyplot as plt","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_name='D10_1.npy'\ndepth=file_name.split('_')[0][1:]\ntemp=np.load('/kaggle/input/test-disks-zscores-x/X/X/'+file_name)\ny_pred=final_model.predict(temp[:576].reshape(1,-1))\ny_pred_reshaped=np.squeeze(y_pred,0)\nrescaled_y_pred_mua=(y_pred_reshaped[:,:,:,0]*factors['mua_range'])+factors['mua_min']\nrescaled_y_pred_mus=(y_pred_reshaped[:,:,:,1]*factors['mus_range'])+factors['mus_min']\n\ntemp_dict=scipy.io.loadmat('/kaggle/input/test-disks/test_disks/'+file_name[:-4]+'.mat')\n#Y=zoom(temp_dict['target'],factors['dwn_factor']+[1],mode='reflect',order=1).astype('float32')\ntemp_y_mua=temp_dict['target'][:,:,:,0]\ntemp_y_mus=temp_dict['target'][:,:,:,1]\ndata_range_mua=np.max([np.max(rescaled_y_pred_mua),np.max(temp_y_mua)])-np.min([np.min(rescaled_y_pred_mua),np.min(temp_y_mua)])\nssim_value_mua = compare_ssim(rescaled_y_pred_mua, temp_y_mua, channel_axis=-1,data_range=data_range_mua)\ndata_range_mus=np.max([np.max(rescaled_y_pred_mus),np.max(temp_y_mus)])-np.min([np.min(rescaled_y_pred_mus),np.min(temp_y_mus)])\nssim_value_mus = compare_ssim(rescaled_y_pred_mus, temp_y_mus, channel_axis=-1,data_range=data_range_mus)\n\n\n'mua'\n''\nlegend_coord=[[0.4,0.78],[0.98,0.78],[0.4,0.2],[0.98,0.2]]\nX, Y, Z = np.mgrid[:16, :16, :8]\n\nfig1 = go.Figure(data=go.Volume(\n    x=X.flatten(), y=Y.flatten(), z=Z.flatten(),\n    value=rescaled_y_pred_mua.flatten(),\n    isomin=np.min(np.min(rescaled_y_pred_mua))+(0*np.min(np.min(rescaled_y_pred_mua))),\n    isomax=np.max(np.max(rescaled_y_pred_mua)),\n    opacity=0.5,\n    surface_count=6,\n    colorbar=dict(len=0.4,x=legend_coord[0][0],y=legend_coord[0][1])\n    ))\nfig1.update_layout(scene_xaxis_showticklabels=False,\n                  scene_yaxis_showticklabels=False,\n                  scene_zaxis_showticklabels=False)\n''\n''\n''\n'Ground truth-mua'\n''\nX, Y, Z = np.mgrid[:16, :16, :8]\n\nfig2 = go.Figure(data=go.Volume(\n    x=X.flatten(), y=Y.flatten(), z=Z.flatten(),\n    value=temp_y_mua.flatten(),\n    isomin=np.min(np.min(temp_y_mua))+(0*np.min(np.min(temp_y_mua))),\n    isomax=np.max(np.max(temp_y_mua)),\n    opacity=0.5,\n    surface_count=6,\n    colorbar=dict(len=0.4,x=legend_coord[1][0],y=legend_coord[1][1])\n    ))\nfig2.update_layout(scene_xaxis_showticklabels=False,\n                  scene_yaxis_showticklabels=False,\n                  scene_zaxis_showticklabels=False)\n''\n''\n''\n'mus'\n''\nX, Y, Z = np.mgrid[:16, :16, :8]\n\nfig3 = go.Figure(data=go.Volume(\n    x=X.flatten(), y=Y.flatten(), z=Z.flatten(),\n    value=rescaled_y_pred_mus.flatten(),\n    isomin=np.min(np.min(rescaled_y_pred_mus))+(0*np.min(np.min(rescaled_y_pred_mus))),\n    isomax=np.max(np.max(rescaled_y_pred_mus)),\n    opacity=0.5,\n    surface_count=6,\n    colorbar=dict(len=0.4,x=legend_coord[2][0],y=legend_coord[2][1])\n    ))\nfig3.update_layout(scene_xaxis_showticklabels=False,\n                  scene_yaxis_showticklabels=False,\n                  scene_zaxis_showticklabels=False)\n''\n''\n''\n'Ground truth-mus'\n''\nX, Y, Z = np.mgrid[:16, :16, :8]\n\nfig4 = go.Figure(data=go.Volume(\n    x=X.flatten(), y=Y.flatten(), z=Z.flatten(),\n    value=temp_y_mus.flatten(),\n    isomin=np.min(np.min(temp_y_mus))+(0*np.min(np.min(temp_y_mus))),\n    isomax=np.max(np.max(temp_y_mus)),\n    opacity=0.5,\n    surface_count=6,\n    colorbar=dict(len=0.4,x=legend_coord[3][0],y=legend_coord[3][1])\n    ))\nfig4.update_layout(scene_xaxis_showticklabels=False,\n                  scene_yaxis_showticklabels=False,\n                  scene_zaxis_showticklabels=False)\n''\n''\n''\nfig = make_subplots(rows=2, cols=2, specs=[[{'type': 'scene'}, {'type': 'scene'}],[{'type': 'scene'},{'type': 'scene'}]])\nfor trace in fig1.data:\n    fig.add_trace(trace, row=1, col=1)\nfor trace in fig2.data:\n    fig.add_trace(trace, row=1, col=2)\nfor trace in fig3.data:\n    fig.add_trace(trace, row=2, col=1)\nfor trace in fig4.data:\n    fig.add_trace(trace, row=2, col=2)\nfig.update_layout(title_text='Depth - '+depth+' mm',width=1000, height=800, annotations=[\n        dict(text=\"Amplitude Only-mua<br>SSIM: {:.6f}\".format(ssim_value_mua), x=0.2, y=1, xref=\"paper\", yref=\"paper\", showarrow=False),\n        dict(text=\"Ground truth-mua\", x=0.8, y=1, xref=\"paper\", yref=\"paper\", showarrow=False),\n        dict(text=\"Amplitude Only-mus<br>SSIM: {:.6f}\".format(ssim_value_mus), x=0.2, y=0.45, xref=\"paper\", yref=\"paper\", showarrow=False),\n        dict(text=\"Ground truth-mus\", x=0.8, y=0.45, xref=\"paper\", yref=\"paper\", showarrow=False)\n    ])\npio.write_image(fig, \"/kaggle/working/Amp_32_\"+depth+\".png\",format=\"png\")\npio.write_image(fig, \"/kaggle/working/Amp_32_\"+depth+\".svg\",format=\"svg\")\nfig.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mua-LOCA & ER\nimport numpy as np\nimport scipy.ndimage as ndi\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef get_lesion_centroids(volume, threshold=0.010):\n    binary_mask = volume > threshold\n    labeled, num_features = ndi.label(binary_mask)\n    centroids = []\n    for i in range(1, num_features + 1):\n        lesion_data = labeled == i\n        coords = np.argwhere(lesion_data)\n        centroid = coords.mean(axis=0)\n        centroids.append(centroid)\n    return centroids\ndef extract_lesion_roi(volume, threshold):\n    binary_mask = volume > threshold\n    labeled, num_features = ndi.label(binary_mask)\n    rois = []\n    for i in range(1, num_features + 1):\n        lesion_data = volume * (labeled == i)\n        if np.any(lesion_data):\n            rois.append(lesion_data)\n    return rois\n\ndef compute_localization_error(predicted_centroid, ground_truth_centroid):\n    return np.linalg.norm(np.array(predicted_centroid) - np.array(ground_truth_centroid))\n\ndef compute_effective_resolution(predicted_roi, ground_truth_centroid):\n    coords = np.argwhere(predicted_roi)\n    distances = [np.linalg.norm(coord - ground_truth_centroid) for coord in coords]\n    max_distance = max(distances)\n    # Diameter of the sphere\n    return 2 * max_distance\n\n# Example usage:\n\npredicted_volume = rescaled_y_pred_mua  # Your predicted volume\nground_truth_volume = temp_y_mua  # Your ground truth volume\n\nthreshold = 0.008  # adjust as per your requirement\npredicted_centroids = get_lesion_centroids(predicted_volume, threshold)\nground_truth_centroids = get_lesion_centroids(ground_truth_volume, threshold)\n\npredicted_rois = extract_lesion_roi(predicted_volume, threshold)\nground_truth_rois = extract_lesion_roi(ground_truth_volume, threshold)\n\nlocalization_errors = []\neffective_resolutions = []\n\nfor i, (predicted_roi, gt_roi) in enumerate(zip(predicted_rois, ground_truth_rois)):\n    print(f\"Processing lesion {i+1}...\")\n\n    # Compute localization error\n    localization_error = compute_localization_error(predicted_centroids[i], ground_truth_centroids[i])\n    print(f\"Localization Error: {localization_error}\")\n    localization_errors.append(localization_error)\n\n    # Compute effective resolution\n    effective_res = compute_effective_resolution(predicted_roi, ground_truth_centroids[i])\n    print(f\"Effective Resolution: {effective_res}\")\n    effective_resolutions.append(effective_res)\n    \nlesions = list(range(1, len(localization_errors)+1))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mua-FWHM\nimport numpy as np\nimport scipy.ndimage as ndi\nfrom scipy.signal import find_peaks\nfrom scipy.interpolate import UnivariateSpline\nimport matplotlib.pyplot as plt\n\ndef extract_lesion_roi(volume, threshold):\n    binary_mask = volume > threshold\n    labeled, num_features = ndi.label(binary_mask)\n    rois = []\n    for i in range(1, num_features + 1):\n        lesion_data = volume * (labeled == i)\n        if np.any(lesion_data):\n            rois.append(lesion_data)\n    return rois\n\ndef extract_profiles(roi):\n    coords = np.argwhere(roi)\n    centroid = coords.mean(axis=0).astype(int)\n    profile_x = roi[:,centroid[1], centroid[2]]\n    profile_y = roi[centroid[0], :, centroid[2]]\n    profile_z= roi[centroid[0], centroid[1],:]\n    return profile_x, profile_y, profile_z\n\ndef compute_fwhm(profile):\n    half_max = np.max(profile) / 2.0\n    spline = UnivariateSpline(range(len(profile)), profile - half_max, s=0)\n    roots = spline.roots()\n    if len(roots) >= 2:\n        return abs(roots[-1] - roots[0])\n    return 0\n\n# Example usage:\nvolume1 = rescaled_y_pred_mua  # Replace with your volume\nvolume2 = temp_y_mua  # Replace with your volume\n\nthreshold = 0.008  # Adjust based on your data\n\nvolumes = [volume1, volume2]\nvolume_name=['Predicted','Ground Truth']\nroi_names=['lesion 1', 'lesion 2']\nfwhms = []\n\nfor volume,vname in zip(volumes,volume_name):\n    rois = extract_lesion_roi(volume, threshold)\n    fwhm_values = []\n\n    for roi,rname in zip(rois,roi_names):\n        profiles = extract_profiles(roi)\n        profile_names = [\"x-profile\", \"y-profile\", \"z-profile\"]\n        \n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        for idx, (profile, name) in enumerate(zip(profiles, profile_names)):\n            if profile.ndim > 1:\n                interpolated_profile = profile.sum(axis=1)\n            else:\n                interpolated_profile = profile\n            fwhm = compute_fwhm(interpolated_profile)\n            fwhm_values.append(fwhm)\n\n            x = np.linspace(0, len(interpolated_profile)-1, 1000)\n            spline = UnivariateSpline(range(len(interpolated_profile)), interpolated_profile - np.max(interpolated_profile) / 2.0, s=0)\n            axes[idx].plot(interpolated_profile, label='Profile')\n            axes[idx].plot(x, spline(x) + np.max(interpolated_profile) / 2.0, 'r--', label='Interpolated')\n            axes[idx].axhline(y=np.max(interpolated_profile)/2.0, color='g', linestyle='--', label='Half Maximum')\n            axes[idx].text(0.6, 0.6, f'FWHM: {fwhm:.2f}', transform=axes[idx].transAxes, fontsize=9, bbox=dict(facecolor='white', alpha=0.6))  # Display FWHM on the graph\n            axes[idx].set_title(name)\n            axes[idx].set_xlabel(\"Position\")\n            axes[idx].set_ylabel(\"Intensity\")\n            legend = axes[idx].legend()\n            \n        \n        fig.suptitle(\"Intensity Profile and Interpolated FWHM - \"+\"Mua - \" + vname+\" - \"+rname)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig(\"amp_mua_\"+f\"{vname}_{rname}.png\")\n        plt.show()\n        plt.close(fig)\n\n    fwhms.append(fwhm_values)\n\nprint(fwhms)\nfig.savefig(\"Amp32_fwhm_mua.png\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mus-LOCA & ER\nimport numpy as np\nimport scipy.ndimage as ndi\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef get_lesion_centroids(volume, threshold=0.010):\n    binary_mask = volume > threshold\n    labeled, num_features = ndi.label(binary_mask)\n    centroids = []\n    for i in range(1, num_features + 1):\n        lesion_data = labeled == i\n        coords = np.argwhere(lesion_data)\n        centroid = coords.mean(axis=0)\n        centroids.append(centroid)\n    return centroids\n\ndef compute_localization_error(predicted_centroid, ground_truth_centroid):\n    return np.linalg.norm(np.array(predicted_centroid) - np.array(ground_truth_centroid))\n\ndef compute_effective_resolution(predicted_roi, ground_truth_centroid):\n    coords = np.argwhere(predicted_roi)\n    distances = [np.linalg.norm(coord - ground_truth_centroid) for coord in coords]\n    max_distance = max(distances)\n    # Diameter of the sphere\n    return 2 * max_distance\n\n# Example usage:\n\npredicted_volume = rescaled_y_pred_mus  # Your predicted volume\nground_truth_volume = temp_y_mus  # Your ground truth volume\n\nthreshold = 0.8 # adjust as per your requirement\npredicted_centroids = get_lesion_centroids(predicted_volume, threshold)\nground_truth_centroids = get_lesion_centroids(ground_truth_volume, threshold)\n\npredicted_rois = extract_lesion_roi(predicted_volume, threshold)\nground_truth_rois = extract_lesion_roi(ground_truth_volume, threshold)\n\nlocalization_errors = []\neffective_resolutions = []\n\nfor i, (predicted_roi, gt_roi) in enumerate(zip(predicted_rois, ground_truth_rois)):\n    print(f\"Processing lesion {i+1}...\")\n\n    # Compute localization error\n    localization_error = compute_localization_error(predicted_centroids[i], ground_truth_centroids[i])\n    print(f\"Localization Error: {localization_error}\")\n    localization_errors.append(localization_error)\n\n    # Compute effective resolution\n    effective_res = compute_effective_resolution(predicted_roi, ground_truth_centroids[i])\n    print(f\"Effective Resolution: {effective_res}\")\n    effective_resolutions.append(effective_res)\n    \nlesions = list(range(1, len(localization_errors)+1))\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#mus-FWHM\nimport numpy as np\nimport scipy.ndimage as ndi\nfrom scipy.signal import find_peaks\nfrom scipy.interpolate import UnivariateSpline\nimport matplotlib.pyplot as plt\n\ndef extract_lesion_roi(volume, threshold):\n    binary_mask = volume > threshold\n    labeled, num_features = ndi.label(binary_mask)\n    rois = []\n    for i in range(1, num_features + 1):\n        lesion_data = volume * (labeled == i)\n        if np.any(lesion_data):\n            rois.append(lesion_data)\n    return rois\n\ndef extract_profiles(roi):\n    coords = np.argwhere(roi)\n    centroid = coords.mean(axis=0).astype(int)\n    profile_x = roi[:,centroid[1], centroid[2]]\n    profile_y = roi[centroid[0], :, centroid[2]]\n    profile_z= roi[centroid[0], centroid[1],:]\n    return profile_x, profile_y, profile_z\n\ndef compute_fwhm(profile):\n    half_max = np.max(profile) / 2.0\n    spline = UnivariateSpline(range(len(profile)), profile - half_max, s=0)\n    roots = spline.roots()\n    if len(roots) >= 2:\n        return abs(roots[-1] - roots[0])\n    return 0\n\n# Example usage:\nvolume1 = rescaled_y_pred_mus  # Replace with your volume\nvolume2 = temp_y_mus  # Replace with your volume\n\nthreshold = 0.8  # Adjust based on your data\n\nvolumes = [volume1, volume2]\nvolume_name=['Predicted','Ground Truth']\nroi_names=['lesion 1', 'lesion 2']\nfwhms = []\n\nfor volume,vname in zip(volumes,volume_name):\n    rois = extract_lesion_roi(volume, threshold)\n    fwhm_values = []\n\n    for roi,rname in zip(rois,roi_names):\n        profiles = extract_profiles(roi)\n        profile_names = [\"x-profile\", \"y-profile\", \"z-profile\"]\n        \n        fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\n        for idx, (profile, name) in enumerate(zip(profiles, profile_names)):\n            if profile.ndim > 1:\n                interpolated_profile = profile.sum(axis=1)\n            else:\n                interpolated_profile = profile\n            fwhm = compute_fwhm(interpolated_profile)\n            fwhm_values.append(fwhm)\n\n            x = np.linspace(0, len(interpolated_profile)-1, 1000)\n            spline = UnivariateSpline(range(len(interpolated_profile)), interpolated_profile - np.max(interpolated_profile) / 2.0, s=0)\n            axes[idx].plot(interpolated_profile, label='Profile')\n            axes[idx].plot(x, spline(x) + np.max(interpolated_profile) / 2.0, 'r--', label='Interpolated')\n            axes[idx].axhline(y=np.max(interpolated_profile)/2.0, color='g', linestyle='--', label='Half Maximum')\n            axes[idx].text(0.6, 0.6, f'FWHM: {fwhm:.2f}', transform=axes[idx].transAxes, fontsize=9, bbox=dict(facecolor='white', alpha=0.6))  # Display FWHM on the graph\n            axes[idx].set_title(name)\n            axes[idx].set_xlabel(\"Position\")\n            axes[idx].set_ylabel(\"Intensity\")\n            legend = axes[idx].legend()\n            \n        \n        fig.suptitle(\"Intensity Profile and Interpolated FWHM - \"+\" Mus - \" + vname+\" - \"+rname)\n        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n        plt.savefig(\"amp_mus_\"+f\"{vname}_{rname}.png\")\n        plt.show()\n        plt.close(fig)\n\n    fwhms.append(fwhm_values)\n\nprint(fwhms)","metadata":{},"execution_count":null,"outputs":[]}]}